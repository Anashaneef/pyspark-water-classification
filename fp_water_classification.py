# -*- coding: utf-8 -*-
"""fp_water_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rUX0pXop3gYU95te9fhzLai6gbGJY0qm
"""

from google.colab import drive
drive.mount('/content/drive')

"""## **Setup Pyspark**"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!tar xf /content/drive/MyDrive/spark-3.0.0-bin-hadoop3.2.tgz

!pip install -q findspark
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.0.0-bin-hadoop3.2"
!pip install pyspark

from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
sc = spark.sparkContext

"""## **Import Library**"""

import numpy as np
import zipfile
from google.colab import files
from pyspark.sql.functions import isnan, when, count, col
from pyspark.ml.linalg import Vectors
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import MultilayerPerceptronClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from sklearn.metrics import classification_report, confusion_matrix

"""## **Preparing dataset from Kaggle**"""

# Install kaggle
!pip install -q kaggle

# Token API
uploaded = files.upload()

# Receive dataset config
!chmod 600 /content/kaggle.json

# Download dataset
! KAGGLE_CONFIG_DIR=/content/ kaggle datasets download -d mssmartypants/water-quality

# extract dataset
local_zip = '/content/water-quality.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/content')
zip_ref.close()

"""## **Dataset Information**

**Variables in the Dataset**

* aluminium - dangerous if greater than 2.8
* ammonia - dangerous if greater than 32.5
* arsenic - dangerous if greater than 0.01
* barium - dangerous if greater than 2
* cadmium - dangerous if greater than 0.005
* chloramine - dangerous if greater than 4
* chromium - dangerous if greater than 0.1
* copper - dangerous if greater than 1.3
* flouride - dangerous if greater than 1.5
* bacteria - dangerous if greater than 0
* viruses - dangerous if greater than 0
* lead - dangerous if greater than 0.015
* nitrates - dangerous if greater than 10
* nitrites - dangerous if greater than 1
* mercury - dangerous if greater than 0.002
* perchlorate - dangerous if greater than 56
* radium - dangerous if greater than 5
* selenium - dangerous if greater than 0.5
* silver - dangerous if greater than 0.1
* uranium - dangerous if greater than 0.3
* is_safe - class attribute {0 - not safe, 1 - safe}
"""

water = spark.read.csv('waterQuality1.csv', header=True, inferSchema=True)
water.show()

"""## **Data Preprocessing**"""

# Count null and NaN value
water.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in water.columns]).show()

water.printSchema()

# Convert tipe data kolom ammonia dari string ke double
water = water.withColumn("ammonia",water.ammonia.cast('double'))

# Convert tipe data kolom is_safe dari string ke double
water = water.withColumn("is_safe",water.is_safe.cast('double'))

water.printSchema()

# Retrieved 7000 data from the entire dataset
water = water.limit(7000)

# View the number of datasets
rows = water.count()
print(f"Dataset berjumlah {rows} baris") # Dataset with {rows} rows

water_df = water.toPandas()
piefreq = water_df['is_safe'].value_counts()
waterdf_piefreq = piefreq.reset_index()
fig = waterdf_piefreq.plot.pie(
    y="is_safe",
)

# Accommodates independent columns in one variable
cols = water.columns[:-1]
cols = np.array(cols)
cols

vecAssembler = VectorAssembler(inputCols = cols, outputCol = "features")
water = water.withColumnRenamed("is_safe","label")

water_df = vecAssembler.transform(water)
water_df = water_df.select(['features', 'label'])
water_df.show(10, truncate=False)

"""## **Modelling and evaluation**

**Modelling**
"""

# Split the data into train and test
splits = water_df.randomSplit([0.9, 0.1], 1234)
train = splits[0]
test = splits[1]

# specify layers for the neural network:
# input layer of size 20 (features), two intermediate of size 34 and 89
# and output of size 2 (classes)
layers = [20, 34, 89, 2]

# create the trainer and set its parameters
trainer = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)

# train the model
model = trainer.fit(train)

# compute accuracy on the test set
result = model.transform(test)
predictionAndLabels = result.select("prediction", "label")
evaluator = MulticlassClassificationEvaluator(metricName="accuracy")
print("Test set accuracy = " + str(evaluator.evaluate(predictionAndLabels)))

result.show()

"""**Evaluation**"""

label = np.array(result.select("label").collect())
prediction = np.array(result.select("prediction").collect())

print(classification_report(label, prediction))

confusion_matrix(label, prediction)

"""**Prediction Test**"""

predict1 = model.predict(Vectors.dense([3,40,0.2,3.2,0.009,6.22,0.5,1.86,2,0.16,0.1,0.2,14,2,0.01,60,7.24,0.65,0.1,0.5]))
if predict1 == 0:
  print('Air tidak aman dikonsumsi') # Water is not safe for consumption
else:
  print('Air aman dikonsumsi') # Water is safe for consumption

predict2 = model.predict(Vectors.dense([2,20,0.005,0.5,0.001,2,0.002,0.57,0.5,0,0,0.001,6.74,0.8,0.001,15,3,0.02,0.04,0.01]))
if predict1 == 0:
  print('Air tidak aman dikonsumsi') # Water is not safe for consumption
else:
  print('Air aman dikonsumsi') # Water is safe for consumption